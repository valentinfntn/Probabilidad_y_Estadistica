\pdfminorversion=4
\documentclass[]{article}

% Packages/Macros %
\usepackage{amssymb,latexsym,amsmath}     % Standard packages
\usepackage{hyperref,comment,enumitem}

% Margins %
\addtolength{\textwidth}{1.0in}
\addtolength{\textheight}{1.00in}
\addtolength{\evensidemargin}{-0.75in}
\addtolength{\oddsidemargin}{-0.75in}
\addtolength{\topmargin}{-.50in}

\newtheorem{theorem}{Teorema}
\newtheorem{proposition}{Proposicion}[theorem]
\newtheorem{definition}{Definicion}
\newenvironment{proof}{\noindent{\bf Prueba:}}{$\hfill \Box$ \vspace{10pt}}
\newenvironment{concept}[1]{\par\noindent\underline{\textbf{#1:}}}{\par\vspace{0.5em}}

\newcommand{\iSection}[1]{
  \phantomsection
  \addcontentsline{toc}{section}{#1}
}
\newcommand{\iSubSection}[1]{
  \phantomsection
  \addcontentsline{toc}{subsection}{#1}
}

\newcommand{\ida}{\Longrightarrow}
\newcommand{\vuelta}{\Longleftarrow}
\newcommand{\cn}[1]{\text{$\mathbb{#1}$}}
\newcommand{\mtx}[2]{\text{$\mathbb{K}^{#1 \times #2}$}}
\newcommand{\incluye}[2]{\text{$#1 \subseteq #2$}}
\newcommand{\conjunto}[1]{\text{$\{ {#1} \}$}}
\newcommand{\tq}{\text{ $|$ }}
\newcommand{\novacio}{\text{$\neq\emptyset$}\:}
\newcommand{\om}{\text{$\Omega$\:}}
\newcommand{\com}[1]{{#1}^{\mathsf{c}}}
\newcommand{\fp}{\mathcal{P}}

\renewcommand{\contentsname}{√çndice}

\begin{document}

\title{Teorico Probabilidad y Estadistica}
\maketitle

\tableofcontents
\newpage

\iSection{Introduccion}
\iSubSection{Conceptos Preliminares}

\begin{concept}{Poblacion}
  el conjunto \textbf{(Total)} sobre el que estamos interesados en obtener conclusiones.
\end{concept}

\begin{concept}{Muestra}
  es un subconjunto de la poblacion al que tenemos acceso y sobre el que realmente hacemos las observaciones.
  Nos interesa principalmente cuando es una \textbf{Muestra Aleatoria Simple}, puede ser \textbf{Con Reemplazo} o \textbf{Sin Reemplazo}.
\end{concept}

\begin{concept}{Experimento Aleatorio}
  conocemos de antemano todos los posibles resultados que se pueden obtener, pueden repetirse indefinidamente en las mismas condiciones.
\end{concept}

\begin{concept}{Espacio Muestral \om}
  el conjunto de todos los posibles resultados distintos de dicho experimento.
\end{concept}

\begin{concept}{Eventos Elementales}
  eventos (subconjunto del espacio muestral) formados por un unico resultado, $A=\conjunto{s}$
\end{concept}

\begin{concept}{Operaciones entre eventos}
  \begin{itemize}
    \item \textbf{Union: } $A\cup B = \conjunto{s \in \om : s \in A \vee s \in B}$
    \item \textbf{Interseccion: } $A\cap B= \conjunto{s \in \om : s \in A \wedge s \in B}$
    \item \textbf{Diferencia: } $A-B=\conjunto{s \in \om : s \in A \wedge s \notin B}$
    \item \textbf{Complemento: } $\com{A} = \conjunto{s \in \om : s \notin A}$
  \end{itemize}
\end{concept}

\newpage
\iSection{Probabilidad}
\iSubSection{Funcion de Probabilidad}

\begin{definition}[Funcion de probabilidad]
  Es una funcion $\mathcal{P}$ que verifica los siguientes axiomas:
  \begin{enumerate}
    \item $\mathcal{P} : \mathbb{P}(\Omega) \to [0,1]$ , $\forall A \subset \Omega \quad 0\leq\mathcal{P}(A)\leq1$
    \item $\mathcal{P}(\Omega)=1$
    \item Si $A_i,A_j$ son eventos disjuntos $(A_i \cap A_j = \emptyset) \ida \mathcal{P}(A_i \cup A_j)= \mathcal{P}(A_i)+\mathcal{P}(A_j)$ 
  \end{enumerate} 
\end{definition}

\begin{concept}{Espacio Equiprobable}
  \begin{itemize}
    \item \om tiene n posibles resultados diferentes
    \item los n resultados tienen la misma probabilidad de aparecer $\frac{1}{n}$
  \end{itemize}
\end{concept}

\iSubSection{Regla de Laplace}
\begin{definition}[Regla de Laplace]
  Si \om es un espacio equiprobable.
  La probabilidad de un evento formado por k eventos elementales $A=\conjunto{a_1,\cdots,a_k}$ es:
  \begin{align*}
    P(A) = \sum_{i=1}^{k} \mathcal{P}(a_i) = \sum_{i=n}^{k} \frac{1}{n}
    = \frac{k}{n} = \frac{\#(A)}{\#(\Omega)} = \frac{\text{casos favorables}}{\text{casos posibles}}
  \end{align*}
\end{definition}

\begin{concept}{Propiedades de la Funcion de Probabilidad}
  \begin{enumerate}
    \item $\fp (\emptyset)=0$
    \item $\fp (\com{A})=1 - \fp(A)$
    \item Si $A \subset B$ entonces $\fp(A)\leq\fp(B)$
    \item $\fp(A\cup B)= \fp(A)+\fp(B)+\fp(A\cap B)$
    \item $\fp(\bigcup^{n}_{i=n}A_i)=\sum_{i=1}^{n}\fp(A_i)-\sum_{i<j}\fp(A_i)+\cdots+(-1)^{n+1}\fp(\bigcap^{n}_{i=1}A_i)$
  \end{enumerate}
\end{concept}

\iSubSection{Variable Aleatoria}
\begin{definition}[Variable Aleatoria]
  Una Variable Aleatoria X es una funcion $X:\Omega \to \mathbb{R}$, que a cada elemento del espacio muestral $(\Omega,\fp)$
  le hace corresponder un numero real.
\end{definition}

\begin{concept}{Rango de la Variable Aleatoria}
  es el conjunto de valores que tienen asociado algun elemento del espacio muestral:
  \begin{align*}
    \Omega_X = \conjunto{x \in \mathbb{R} : \exists s \in \Omega, X(s)=x}
  \end{align*} 
\end{concept}

\iSubSection{Funcion de Masa}
\begin{definition}[Funcion de Masa Discreta]
  Es la funcion que representa la probabilidad de que X tome cada uno de los valores posibles $x_i$ :
  \begin{align*}
    &p : \mathbb{R} \to [0,1]\\
    &x_i \to p(x_i) = P(X=x_i) = \fp(s \in \Omega : X(s)=x_i)
  \end{align*} 
\end{definition}

\begin{concept}{Propiedades de la Funcion de Masa}
  \begin{enumerate}
    \item $0\leq p(x) \leq 1$ , $\forall x \in \mathbb{R}$
    \item $\sum_{i \in I} p(x_i)= 1$
    \item Si $A \subset \mathbb{R}$ , $P(X \in A) = \sum_{x_i \in A} p(x_i)$
  \end{enumerate}
\end{concept}

\iSubSection{Funcion de Distribucion}
\begin{concept}{Funcion de Distribucion}
  \begin{align*}
    F(x)=P(-\infty,x]= P(s \in \Omega : X(s)\leq x) , \forall x \in \mathbb{R}    
  \end{align*}
Se deducen las siguientes propiedades:
  \begin{enumerate}
    \item $\lim_{x \to -\infty} F(x)=0$
    \item $\lim_{x \to \infty} F(x)=1$
    \item Si $x_1 \leq x_2 \ida F(x_1)\leq F(x_2)$
    \item $F(x^+)=\lim_{h \to 0^+} F(x + h) = F(x)$
    \item $P(a,b]=F(a)-F(b)$
  \end{enumerate}
\end{concept}

\begin{definition}[Funcion de Distribucion Discreta]
  \begin{align*}
    F(x)= \sum_{x_i < x} P(X = x_i) = \sum_{x_i < x} p(x_i)
  \end{align*}
\end{definition}

\newpage
\iSection{Probabilidad Condicional}

\iSubSection{Independencia}
\begin{definition}[Independencia]
  Diremos que A y B son independientes sii:
  \begin{align*}
    \fp(A \cap B) =  \fp(A)\cdot \fp(B)
  \end{align*}
\end{definition}

\iSubSection{Evento Condicionado}
\begin{definition}[Evento Condcionado]
  Si la probabilidad de un evento A esta condicionada por un evento B se da que:
  \begin{align*}
    \fp(A | B)=\frac{\fp(A\cap B)}{\fp(B)}
  \end{align*} 
\end{definition}

\begin{concept}{Particion sobre \om}
  \begin{enumerate}
    \item Eventos exhaustivos: $\Omega = \bigcup^{r}_{i=1}B_i$
    \item Eventos excluyentes: $B_i \cap B_j = \emptyset$ para todo $i \neq j$
  \end{enumerate}
\end{concept}

\iSubSection{Regla de Probabilidad Total}
\begin{definition}[Regla de Probabilidad Total]
  A partir de una particion podemos calcular A como la union de todas las intersecciones con los eventos B.
  \begin{align*}
    &\text{Sea } A = (A\cap B_1) \cup\cdots\cup (A\cap B_r)\\
    &\fp(A)= \fp(A\cap B_1) + \cdots + \fp(A\cap B_r)\\
    &\fp(A)= \fp(A | B_1) \cdot \fp(B_1) + \cdots + \fp(A | B_r) \cdot \fp(B_r)\\
    &\ida \quad \fp(A)= \sum_{i=1}^{r} \fp(A | B_i) \cdot \fp(B_i)
  \end{align*}
\end{definition}

\iSubSection{Regla de Bayes}
\begin{definition}[Regla de Bayes]
  Del mismo modo a partir de una particion de espacio muestral se deduce que:
  \begin{align*}
    &\fp(B_j | A)= \frac{\fp(B_j \cap A)}{\fp(A)} = \frac{\fp(B_j \cap A)}{\sum_{i=1}^{r} \fp(A | B_i)\cdot \fp(B_i)}
  \end{align*}
\end{definition}

\iSection{Esperanza}
\begin{definition}[Esperanza Matematica]
  Denotamos esperanza matematica (o media) a la tendencia central mas importante:
  \begin{align*}
    &E[X]= \sum_{i=1}^{n} x_i p(x_i)
  \end{align*}
\end{definition}

\begin{concept}{Propiedades}
  Sean $X,Y$ variables aleatorias y sean $a,b,c \in \mathbb{R}$ constantes entonces:
  \begin{itemize}
    \item $E[aX+b] = aE[X]+b$
    \item $E[X+Y] = E[X] + E[Y]$
    \item $E[c] = c$
    \item $E[cX] = cE[X]$
    \item Si $X \geq 0$ entonces $E[X] \geq 0$
    \item Si $X \leq Y$ entonces $E[X] \leq E[Y]$
  \end{itemize}
\end{concept}

\iSection{Varianza}
\begin{definition}[Varianza]
  Representa la distancia cuadratica promedio a la media, se denota
  $V(X) = E[(X-\mu_x)^2] = E[X^2]-E[X]^2$. Tambien denota a la raiz cuadrada  de la desviacion tipica ($\sigma_X$),
  por lo tanto $V(X)=\sigma^{2}_X$.
\end{definition}

\iSection{Distribuciones Discretas}

\iSubSection{Uniforme Discreta}
\begin{definition}[Uniforme: $X \equiv Unif\conjunto{x_1,\cdots,x_n}$]
  Una variable X tiene distribucion\\ uniforme discreta sobre el conjunto
  \conjunto{x_1,\cdots,x_n} si la probabilidad de que tome cualquiera de los valores es la misma.
  \begin{itemize}
    \item \underline{Funcion de masa:}
    \begin{align*}
      P(X = x_i) =
      \begin{cases}
        \frac{1}{n} \text{ si } i \in \conjunto{x_1,\cdots,x_n}\\
        0 \text{ en caso contrario}
      \end{cases}
    \end{align*}
    \item \underline{Esperanza:}
    \begin{align*}
      E[X] = \frac{1}{n}\sum_{i=1}^{n} x_i
    \end{align*}
    \item \underline{Varianza:}
    \begin{align*}
      V[X] = \frac{1}{n}\sum_{i=1}^{n} x_i - (\frac{1}{n}\sum_{i=1}^{n} x_i)^2
    \end{align*}
  \end{itemize}
\end{definition}

\iSubSection{Bernoulli}
\begin{definition}[Bernoulli: $X\equiv Bern(p)$]
  Una distribucion Bernoulli se caracteriza por tener solo dos posibles resultados, "exito" o "fracaso".
  Con probabilidades $p$ y $(1-p)$ respectivamente.
  \begin{itemize}
    \item \underline{Funcion de masa:}
    \begin{align*}
      P(X = x) = p^x \cdot (1-p)^{1-x}
      \text{ donde $X$} =
      \begin{cases}
        1 \text{ si obtenemos un "exito"}\\
        0 \text{ si obtenemos un "fracaso"}
      \end{cases}
    \end{align*}
    \item \underline{Esperanza:}
    \begin{align*}
      E[X] = p
    \end{align*}
    \item \underline{Varianza:}
    \begin{align*}
      V[X] = p \cdot (1-p)
    \end{align*}
  \end{itemize}
\end{definition}

\newpage
\iSubSection{Binomial}
\begin{definition}[Binomial: $\mathcal{B}(n,p)$]
  Se genera por la repeticion de n pruebas de Bernoulli independientes. En esta distribucion
  X="numero de exitos obtenidos en las n pruebas de Bernoulli".\\
  En este caso $x_i$ vale "1" si obtenemos "exito" en la i-esima prueba  y vale "0" en el caso contrario.
  \begin{itemize}
    \item \underline{Funcion de masa:}
    \begin{align*}
      P(X = x) = \binom{n}{x} \cdot p^x \cdot (1-p)^{1-xs}
    \end{align*}
    \item \underline{Esperanza:}
    \begin{align*}
      E[X] = n \cdot p
    \end{align*}
    \item \underline{Varianza:}
    \begin{align*}
      V[X] = n \cdot (1-p) \cdot p 
    \end{align*}
  \end{itemize}
\end{definition}


\iSubSection{Poisson}
\begin{definition}[Poisson: $\mathbb{P}(\lambda)$]
  Expresa la probabilidad de que ocurran un numero de $X$ sucesos en un tiempo fijo, ocurren con
  una tasa media conocida ($\lambda$) y son independientes del tiempo transcurrido desde el ultimo suceso.\\
  Se aplican a eventos con probabilidad muy baja de ocurrir, pues esta distribucion se aplica sobre un intervalo
  continuo numerable.
  \begin{itemize}
    \item \underline{Funcion de masa:}
    \begin{align*}
      P(X = x) = \frac{e^\lambda \cdot \lambda^x}{x!} \text{ con } x=0,1,\cdots
    \end{align*}
    \item \underline{Esperanza:}
    \begin{align*}
      E[X] = \lambda
    \end{align*}
    \item \underline{Varianza:}
    \begin{align*}
      V[X] = \lambda
    \end{align*}
  \end{itemize}
\end{definition}

\begin{concept}{Propiedad (Infinitamente Divisible)}
  Si $X_i \equiv \mathbb{P}(\lambda_i)$ son independientes, entonces la v.a. :
  \begin{align*}
    X = \sum_{i}X_i \equiv \mathbb{P}(\sum_{i} \lambda_i)
  \end{align*}
\end{concept}

\iSubSection{Hipergeometrica}
\begin{definition}[Hipergeometrica: $\mathcal{H}(r_1,r,n)$]
  Consideramos una poblacion de r objetos los cuales $r_1$ son de un tipo y $r_2=r-r_1$ son de otro.\\
  Extraemos de esta poblacion un subconjunto de n objetos al azar, sin reposicion y sin considerar el orden
  de extraccion.
  \begin{itemize}
    \item \underline{Funcion de masa:}
    \begin{align*}
      P(X = X) = \frac{\binom{r_1}{x} \cdot \binom{r-r_1}{n-x}}{\binom{r}{n}} 
    \end{align*}
    \item \underline{Esperanza:}
    \begin{align*}
      E[X] = \frac{n \cdot r_1}{r}
    \end{align*}
    \item \underline{Varianza:}
    \begin{align*}
      V[X] = \frac{r_1 \cdot n \cdot (r-r_1) \cdot (r-n)}{r^2 \cdot (r-1)}
    \end{align*}
  \end{itemize}
\end{definition}

\newpage
\iSubSection{Geometrica}
\begin{definition}[Geometrica: $\mathcal{G}(p)$]
  Consiste en repetir una serie de experimentos Bernoulli hasta obtener el primer "exito".\\
  Si  X toma el valor k, entonces tenemos $k-1$ fracasos y $1$ exito.
  \begin{itemize}
    \item \underline{Funcion de masa:}
    \begin{align*}
      P(X = x) = (1-p)^{k-1}\cdot p \text{ con } k=1,2,3,\cdots
    \end{align*}
    \item \underline{Esperanza:}
    \begin{align*}
      E[X] = \frac{1}{p}
    \end{align*}
    \item \underline{Varianza:}
    \begin{align*}
      V[X] = \frac{(1-p)}{p^2}
    \end{align*}
  \end{itemize}
\end{definition}


\iSubSection{Binomial Negativa}
\begin{definition}[Binomial Negativa: $\mathcal{BN}(n,p)$]
  Supongamos que tenemos una sucesion un experimento de Bernoulli independientes.
  Consideramos el evento donde realizamos k experimentos hasta obtener r "exitos".\\
  Toda secuencia de experimentos de ese evento tiene la misma probabilidad de aparecer y
  es de la forma $EF\cdots E$ (siempre hay un exito en la ultima posicion).\\
  Por lo tanto la cantidad se secuencias posibles es la forma de acomodar los $r-1$ exitos
  en los $k-1$ lugares restantes.
  \begin{itemize}
    \item \underline{Funcion de masa:}
    \begin{align*}
      P(X = k) = \binom{k-1}{r-1} \cdot (1-p)^{k-r} \cdot p^r \text{ con } k=r,(r+1),(r+2),\cdots
    \end{align*}
    \item \underline{Esperanza:}
    \begin{align*}
      E[X] = \frac{r}{p}
    \end{align*}
    \item \underline{Varianza:}
    \begin{align*}
      V[X] = r\cdot \frac{(1-p)}{p^2}
    \end{align*}
  \end{itemize}
\end{definition}

\end{document}
